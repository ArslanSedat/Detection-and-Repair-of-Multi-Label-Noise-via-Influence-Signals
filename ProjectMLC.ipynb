!pip install CleanLab
!pip install Cleanlab[datalab]
import numpy as np
import tensorflow as tf
from sklearn.datasets import make_multilabel_classification
from sklearn.model_selection import train_test_split
from PIL import Image
import tensorflow_datasets as tfds
import numpy as np
import random
import tensorflow as tf
from sklearn.model_selection import train_test_split
from PIL import Image
import tensorflow_datasets as tfds
from tensorflow.keras import optimizers
from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
import sklearn.base
from cleanlab.filter import find_label_issues
from cleanlab import Datalab
from cleanlab.internal.multilabel_utils import int2onehot, onehot2int
from PIL import Image
!pip install cleanlab scikit-learn
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import average_precision_score
from cleanlab.multilabel_classification.filter import find_label_issues
from cleanlab.multilabel_classification.filter import find_multilabel_issues_per_class
from cleanlab.benchmarking.noise_generation import generate_noise_matrix_from_trace
from cleanlab.benchmarking.noise_generation import generate_noisy_labels
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision import models as torch_models
import torchvision.transforms as T





############################################################
# Load VOC data and baseline training (TensorFlow)
############################################################
data, info = tfds.load("voc/2007", split="train[:100%]", with_info=True)
label_names = info.features["objects"]["label"].names
num_labels = len(label_names)

binary_labels = []
images = []

for sample in data:
    labels = np.zeros(num_labels, dtype=int)
    for label_id in sample["objects"]["label"].numpy():
        labels[label_id] = 1
    binary_labels.append(labels)
    image = Image.fromarray(sample["image"].numpy())
    image = image.resize((224, 224))
    images.append(np.array(image))

binary_labels = np.array(binary_labels)
images = np.array(images, dtype='float32') / 255.0

X_full_train, X_test, y_full_train, y_test = train_test_split(images, binary_labels, test_size=0.2, random_state=42)

def build_model(num_labels):
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(224,224,3)),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Conv2D(256, (3,3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2,2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(num_labels, activation='sigmoid')
    ])
    model.compile(optimizer=optimizers.Adam(1e-4),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

print("Training baseline model on clean data...")
model_clean = build_model(num_labels)
model_clean.fit(X_full_train, y_full_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)
clean_loss, clean_acc = model_clean.evaluate(X_test, y_test, verbose=0)
print("Clean baseline accuracy:", clean_acc)

############################################################
# Introduce Noise and Retrain
############################################################
noise_rate = 0.2
num_samples = len(y_full_train)
num_noisy = int(num_samples * noise_rate)
np.random.seed(42)
noisy_indices = np.random.choice(num_samples, num_noisy, replace=False)

y_noisy = y_full_train.copy()
for idx in noisy_indices:
    current_labels = y_noisy[idx]
    pos_indices = np.where(current_labels == 1)[0]
    if len(pos_indices) > 0:
        flip_label = np.random.choice(pos_indices)
        y_noisy[idx, flip_label] = 0
    else:
        flip_label = np.random.randint(num_labels)
        y_noisy[idx, flip_label] = 1

print(f"Introduced noise in {num_noisy} samples.")

model_noisy = build_model(num_labels)
model_noisy.fit(X_full_train, y_noisy, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)
noisy_loss, noisy_acc = model_noisy.evaluate(X_test, y_test, verbose=0)
print("Noisy dataset accuracy:", noisy_acc)

############################################################
# Cleanlab Detection and Correction
############################################################
labels_list = []
for row in y_noisy:
    lbls = np.where(row == 1)[0].tolist()
    if len(lbls) == 0:
        lbls = [np.random.randint(num_labels)]
    labels_list.append(lbls)

num_class = num_labels
X_train_flat = X_full_train.reshape(len(X_full_train), -1)
y_onehot = y_noisy

SEED = 0
random.seed(SEED)
single_class_labels = [random.choice(i) for i in labels_list]
clf = OneVsRestClassifier(RandomForestClassifier(random_state=SEED))
pred_probs = np.zeros(shape=(len(labels_list), num_class))
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

for train_index, test_index in kf.split(X=X_train_flat, y=single_class_labels):
    clf_cv = sklearn.base.clone(clf)
    X_train_cv, X_test_cv = X_train_flat[train_index], X_train_flat[test_index]
    y_train_cv, y_test_cv = y_onehot[train_index], y_onehot[test_index]
    clf_cv.fit(X_train_cv, y_train_cv)
    y_pred_cv = clf_cv.predict_proba(X_test_cv)
    pred_probs[test_index] = y_pred_cv

lab = Datalab(
    data={"labels": labels_list},
    label_name="labels",
    task="multilabel",
)
lab.find_issues(
    pred_probs=pred_probs,
    issue_types={"label": {}}
)
label_issues = lab.get_issues("label")
issues = label_issues.query("is_label_issue").index.values

detected_set = set(issues)
noisy_set = set(noisy_indices)

tp = len(detected_set.intersection(noisy_set))
fp = len(detected_set - noisy_set)
fn = len(noisy_set - detected_set)
tn = len(set(range(num_samples)) - (noisy_set.union(detected_set)))

precision = tp / (tp+fp) if (tp+fp)>0 else 0.0
recall = tp / (tp+fn) if (tp+fn)>0 else 0.0
f1 = (2*precision*recall)/(precision+recall) if (precision+recall)>0 else 0.0

print("Cleanlab Detection Performance:")
print(f" Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")

mask = np.ones(num_samples, dtype=bool)
mask[issues] = False
X_corrected = X_full_train[mask]
y_corrected = y_noisy[mask]

model_corrected = build_model(num_labels)
model_corrected.fit(X_corrected, y_corrected, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=1)
corrected_loss, corrected_acc = model_corrected.evaluate(X_test, y_test, verbose=0)
print("Post-Cleanlab correction accuracy:", corrected_acc)

############################################################
# Holistic Label Correction (HLC) with GCN
############################################################

class CustomDatasetTorch(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        label = self.labels[idx]
        img_pil = Image.fromarray((image*255).astype(np.uint8))
        if self.transform:
            image = self.transform(img_pil)
        else:
            image = transforms.ToTensor()(img_pil)
        return image, label

class DynamicGraphConvolution(nn.Module):
    def __init__(self, in_features, out_features, num_nodes):
        super(DynamicGraphConvolution, self).__init__()

        self.static_adj = nn.Sequential(
            nn.Conv1d(num_nodes, num_nodes, 1, bias=False),
            nn.LeakyReLU(0.2))
        self.static_weight = nn.Sequential(
            nn.Conv1d(in_features, out_features, 1),
            nn.LeakyReLU(0.2))

        self.gap = nn.AdaptiveAvgPool1d(1)
        self.conv_global = nn.Conv1d(in_features, in_features, 1)
        self.bn_global = nn.BatchNorm1d(in_features)
        self.relu = nn.LeakyReLU(0.2)

        self.conv_create_co_mat = nn.Conv1d(in_features*2, num_nodes, 1)
        self.dynamic_weight = nn.Conv1d(in_features, out_features, 1)

    def forward_static_gcn(self, x):
        x = self.static_adj(x.transpose(1, 2))
        x = self.static_weight(x.transpose(1, 2))
        return x

    def forward_construct_dynamic_graph(self, x):
        x_glb = self.gap(x)
        x_glb = self.conv_global(x_glb)
        x_glb = self.bn_global(x_glb)
        x_glb = self.relu(x_glb)
        x_glb = x_glb.expand(x_glb.size(0), x_glb.size(1), x.size(2))
        x = torch.cat((x_glb, x), dim=1)
        dynamic_adj = self.conv_create_co_mat(x)
        dynamic_adj = torch.sigmoid(dynamic_adj)
        return dynamic_adj

    def forward_dynamic_gcn(self, x, dynamic_adj):
        x = torch.matmul(x, dynamic_adj)
        x = self.relu(x)
        x = self.dynamic_weight(x)
        x = self.relu(x)
        return x

    def forward(self, x):
        out_static = self.forward_static_gcn(x)
        x = x + out_static
        dynamic_adj = self.forward_construct_dynamic_graph(x)
        x = self.forward_dynamic_gcn(x, dynamic_adj)
        return dynamic_adj, x

class ADD_GCN(nn.Module):
    def __init__(self, model, num_classes):
        super(ADD_GCN, self).__init__()
        self.features = nn.Sequential(
            model.conv1,
            model.bn1,
            model.relu,
            model.maxpool,
            model.layer1,
            model.layer2,
            model.layer3,
            model.layer4,
        )
        self.num_classes = num_classes

        self.fc = nn.Conv2d(model.fc.in_features, num_classes, (1, 1), bias=False)

        self.conv_transform = nn.Conv2d(2048, 1024, (1, 1))
        self.relu = nn.LeakyReLU(0.2)

        self.gcn = DynamicGraphConvolution(1024, 1024, num_classes)

        self.mask_mat = nn.Parameter(torch.eye(self.num_classes).float())
        self.last_linear = nn.Conv1d(1024, self.num_classes, 1)

    def forward_feature(self, x):
        x = self.features(x)
        return x

    def forward_classification_sm(self, x):
        x = self.fc(x)
        x = x.view(x.size(0), x.size(1), -1)
        x = x.topk(1, dim=-1)[0].mean(dim=-1)
        return x

    def forward_sam(self, x):
        mask = self.fc(x)
        mask = mask.view(mask.size(0), mask.size(1), -1)
        mask = torch.sigmoid(mask)
        mask = mask.transpose(1, 2)

        x = self.conv_transform(x)
        x = x.view(x.size(0), x.size(1), -1)
        x = torch.matmul(x, mask)
        return x

    def forward_dgcn(self, x):
        return self.gcn(x)

    def forward(self, x):
        x = self.forward_feature(x)
        out1 = self.forward_classification_sm(x)
        v = self.forward_sam(x)
        dag, dag_ = self.forward_dgcn(v)
        z = v + dag_
        out2 = self.last_linear(z)
        mask_mat = self.mask_mat.detach()
        out2 = (out2 * mask_mat).sum(-1)
        return torch.sigmoid((out1 + out2) / 2), dag


############################################################
# Train of HLC model on corrected dataset
############################################################

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

hlc_transform = T.Compose([
    T.Resize((224,224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

hlc_train_dataset = CustomDatasetTorch(X_corrected, y_corrected, transform=hlc_transform)
hlc_test_dataset = CustomDatasetTorch(X_test, y_test, transform=hlc_transform)

hlc_train_loader = DataLoader(hlc_train_dataset, batch_size=32, shuffle=True)
hlc_test_loader = DataLoader(hlc_test_dataset, batch_size=32, shuffle=False)

hlc_model = ADD_GCN(torch_models.resnet50(pretrained=True), len(label_names))
hlc_model.to(device)

hlc_criterion = nn.BCELoss()
hlc_optimizer = torch.optim.Adam(hlc_model.parameters(), lr=1e-4)
hlc_num_epochs = 10

for epoch in range(hlc_num_epochs):
    hlc_model.train()
    epoch_loss = 0
    for images_batch, labels_batch in hlc_train_loader:
        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device).float()
        hlc_optimizer.zero_grad()
        outputs, _ = hlc_model(images_batch)
        loss = hlc_criterion(outputs, labels_batch)
        loss.backward()
        hlc_optimizer.step()
        epoch_loss += loss.item()
    print(f"HLC Epoch {epoch+1}/{hlc_num_epochs}, Loss: {epoch_loss/len(hlc_train_loader):.4f}")

hlc_model.eval()
y_pred_list = []
y_true_list = []

with torch.no_grad():
    for images_batch, labels_batch in hlc_test_loader:
        images_batch = images_batch.to(device)
        outputs, _ = hlc_model(images_batch)
        y_pred_list.append(outputs.cpu().numpy())
        y_true_list.append(labels_batch.numpy())

y_pred = np.vstack(y_pred_list)
y_true = np.vstack(y_true_list)

ap_scores = []
f1_scores = []
y_pred_binary = (y_pred > 0.5).astype(int)
for i in range(num_labels):
    ap = average_precision_score(y_true[:, i], y_pred[:, i])
    ap_scores.append(ap)
    f1_i = f1_score(y_true[:, i], y_pred_binary[:, i])
    f1_scores.append(f1_i)
hlc_map = np.mean(ap_scores)
hlc_f1 = np.mean(f1_scores)
hlc_accuracy = accuracy_score(y_true.flatten(), y_pred_binary.flatten())

print("\nSummary of results:")
print(f"Clean baseline accuracy (TF): {clean_acc:.3f}")
print(f"Noisy data accuracy (TF): {noisy_acc:.3f}")
print(f"Post-Cleanlab correction accuracy (TF): {corrected_acc:.3f}")

print("Cleanlab detection metrics:")
print(f" Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")

print("\nHLC Results (GCN-based):")
print(f"mAP after HLC correction: {hlc_map:.3f}")
print(f"Average F1-score after HLC correction: {hlc_f1:.3f}")
print(f"Accuracy after HLC correction: {hlc_accuracy:.3f}")
